* MNIST 데이터셋
 - MNIST 데이터셋은 Yann LeCun의 웹사이트(http://yann.lecun.com/exdb/mnist/)에 호스팅되어 있다.
->데이터를 다운로드받고 설치하는 파이썬 코드
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

 - 다운로드 된 데이터는 세 부분으로 나뉜다.
1) 55,000개의 학습 데이터(mnist.train)
2) 10,000개의 테스트 데이터(mnist.text)
3) 5,000개의 검증 데이터(mnist.validation)

 - 각 MNIST 데이터셋은 두 부분으로 나뉜다. 
1) 손으로 쓴 숫자
2) 그에 따른 라벨
 - xs : 이미지
   ys : 라벨
 학습 데이터셋과 테스트 데이터셋은 둘 다 xs와 ys를 가진다. 
  예를 들어, 학습 이미지: mnist.train.images, 학습 라벨: mnist.train.labels


 - 각 이미지가 28x28 픽셀이라고 하면, 이 배열을 펼쳐서 28x28 = 784 개의 벡터로 만들 수 있다. (이미지들간에 일관적으로 처리하기만한다면, 배열을 어떻게 펼치든지 상관 없다.)
 -> MNIST 이미지는 매우 호화스러운 구조 즉, 연산을 많이 요하는 시각화를 가진, 단지 784차원 벡터 공간에 있는 여러 개의 데이터일 뿐이다.


 - 데이터를 펼친 결과로 mnist.train.images는 [55000, 784]의 형태를 가진 텐서(n차원 배열)가 된다. 
 1) 첫 번째 차원: 이미지
 2) 두 번째 차원: 각 이미지의 픽셀
 텐서의 모든 성분은 특정 이미지의 특정 픽셀을 특정하는 0과 1사이의 픽셀 강도

 - MNIST에서 각각에 대응하는 라벨은 0과 9사이의 숫자, 
   각 이미지가 어떤 숫자인지를 말해준다. 

 - 이 튜토리얼의 목적을 위해서 우리는 라벨을 "원-핫 벡터"로 바꾸길 원한다. 
   원-핫 벡터는 단 하나의 차원에서만 1이고, 나머지 차원에서는 0인 벡터이다. 
   이 경우, n번째 숫자는 n번째 차원이 1인 벡터로 표현될 것 ex) 3은 [0,0,0,1,0,0,0,0,0,0]. 
   결과적으로, mnist.train.labels는 [55000, 10]의 모양을 같은 실수 배열이 된다.
      ->정수 배열이 아니라, 실수 배열로 취급하는 데에는 이후 소프트맥스 회귀의 결과가 정수형이 아닌 실수형으로 산출되기 때문






* 소프트맥스 회귀(softmax regression)
 - MNIST의 각 이미지가 0부터 9 사이의 손으로 쓴 숫자이므로 각 이미지는 10가지의 경우의 수 중 하나이고 이미지를 보고 그 이미지가 각 숫자일 확률을 계산할 것이다. 
 ex) 9가 쓰여져 있는 이미지를 보고 이 이미지가 80%의 확률로 9라고 추측하지만, 윗쪽의 동그란 부분 때문에 8일 확률도 5% 있다고 계산할 수도 있다. 또한 확실하지 않기 때문에 그 외의 다른 숫자일 확률도 조금씩 있을 수 있다. 
   -> 소프트맥스 회귀를 사용하기에 아주 적절한 예

 - 소프트맥스를 쓰는 이유
  : 만약 어떤 것이 서로 다른 여러 항목 중 하나일 확률을 계산하고자 할 때.
   소프트맥스는 각 값이 0과 1 사이의 값이고, 각 값을 모두 합하면 1이 되는 목록을 제공함
  게다가 나중에 더 복잡한 모델을 트레이닝 할 때에도, 마지막 단계는 소프트맥스 레이어가 될 것.

 - 소프트맥스 회귀의 두 단계
 1. 입력한 데이터가 각 클래스에 속한다는 증거(evidence)를 수치적으로 계산 
 2. 계산한 값을 확률로 변환

 - 가중치 합(weighted sum): 서로 다른 계수를 곱해 합하는 계산

 - 한 이미지가 특정 클래스에 속하는지 계산하기 위해서는 각 픽셀의 어두운 정도(intensity)를 가중치합 한다. 

 - 여기서의 '가중치'
 1) 양(+): 해당 픽셀이 진하다는 것이 특정 클래스에 속한다는 내용
 2) 음(-): 해당 픽셀이 진하다는 것이 특정 클래스에 속한다는 것에 반하는 내용
 - ex) 빨간 부분: 음의 가중치, 파란 부분: 양의 가중치
   여기서 바이어스(bias)라는 추가적인 항을 더하게 된다. 
   결과값의 일부는 입력된 데이터와는 독립적일 수 있다는 것을 고려하기 위함

 - \(x\) : 입력값
   \(i\) : 클래스
   \(W_i\) : 가중치
   \(b_i\) : 클래스 \(i\)에 대한 바이어스
   \(j\) : 입력 데이터로 사용한 이미지 \(x\)의 픽셀 값을 합하기 위한 인덱스
이제 각 클래스에 대해 계산한 증거값들을 "소프트맥스" 함수를 활용해 예측 확률 \(y\)로 변환한다.

 - 여기서의 소프트맥스: "활성화" 또는 "링크" 함수의 역할
   ->계산한 선형 함수를 우리가 원하는 형태 ( 이 경우에서는 10가지 경우에 대한 확률 분포 ) 로 변환하는데 사용하는 것
   계산한 증거값들: 입력된 데이터 값이 각 클래스에 속할 확률로 변환하는 것

 
 - 지수화: 증거값을 하나 더 추가하면 어떤 가설에 대해 주어진 가중치를 곱으로 증가시키는 것

 - 식이 많은 경우, 소프트맥스를 입력값을 지수화한 뒤 정규화 하는 과정이라고 생각하는게 편하다. 
   또한 반대로, 증거값의 갯수가 하나 줄어든다는 것은 가설의 가중치가 기존 가중치의 분수비로 줄어들게 된다는 뜻이다. (어떤 가설도 0 또는 음의 가중치를 가질 수 없다.)
  소프트맥스는 가중치를 정규화한 후, 모두 합하면 1이 되는 확률 분포로 만든다. 


 - 소프트맥스 회귀 : 각각의 출력값에 대해, 가중치합을 계산하고 바이어스를 더한 뒤 소프트맥스를 적용하는 것






* 회귀 구현하기
 
 - 파이썬에서의 수치 연산
 : 파이썬에서 하나의 무거운 작업을 독립적으로 실행한다.
  ->매 연산마다 파이썬으로 다시 돌아오는 과정에서 많은 오버헤드가 발생할 수 있다. 이러한 오버헤드는 GPU에서 연산을 하거나 분산 처리 환경같은, 데이터 전송에 큰 비용이 발생할 수 있는 상황에서 특히 문제가 될 수 있다.

 - 텐서플로우에서의 수치 연산
 : 서로 상호작용하는 연산간의 그래프를 유저가 기술하도록 하고, 그 연산 모두가 파이썬 밖에서 동작한다. (이러한 접근 방법은 다른 몇몇 머신러닝 라이브러리에서 볼 수 있다).


 - 텐서플로우를 사용하기 위한 코드
import tensorflow as tf

 - 상호작용하는 연산들을 심볼릭 변수를 활용해 기술하게 되는 코드
x = tf.placeholder(tf.float32, [None, 784])

x에 특정한 값이 주어진 것은 아니다. 
placeholder: 텐서플로우에서 연산을 실행할 때 값을 입력할 자리
 -> 여기서는 784차원의 벡터로 변형된 MNIST 이미지의 데이터를 넣으려고 한다.
   이걸 [None, 784]의 형태를 갖고 부동소수점으로 이루어진 2차원 텐서로 표현한다. (여기서 None은 해당 차원의 길이가 어떤 길이든지 될 수 있음을 의미한다)

 - 가중치와 바이어스 역시 필요
Variable: 서로 상호작용하는 연산으로 이루어진 텐서플로우 그래프 안에 존재하는, 수정 가능한 텐서
 연산에 사용되기도 하고, 연산을 통해 수정되기도 한다.
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
 -> tf.Variable에 Variable의 초기값을 넘겨줌으로써 이 Variable들을 생성한다.
 여기서는 W와 b 둘 다 0으로 이루어진 텐서로 초기화를 한다. 
 이제부터 W와 b를 학습해 나갈 것이므로, 각각의 초기값은 크게 중요하지 않다.

 - W가 [784, 10]의 형태를 갖는 것의 이유
   : W에 784차원의 이미지 벡터를 곱해서 각 클래스에 대한 증거값을 나타내는 10차원 벡터를 얻고자 하기 때문
    b는 그 10차원 벡터에 더하기 위해 [10]의 형태를 갖는 것

 - 모델을 구현 코드
y = tf.nn.softmax(tf.matmul(x, W) + b)
 -> tf.matmul(x, W)로 x와 W를 곱한다. 이 표현은 위에서 본 수식에서 곱했던 순서인 \(Wx\)와 반대인데, x가 여러 입력값을 갖는 2차원 텐서인 경우에도 대응하기 위한 작은 트릭이다. 
    그 다음엔 b를 더하고, 마지막으로 tf.nn.softmax을 적용한다.

 - 간단하게 구현할 수 있는 이유
  : 텐서플로우는 머신러닝 모델에서부터 물리학 시뮬레이션까지 다양한 종류의 수치 연산을 표현할 수 있는 매우 유연한 방법이다. 게다가 한 번 작성한 모델은 여러 기기에서 실행할 수 있다(컴퓨터에 있는 CPU, GPU, 휴대폰).






* 학습 
1) 모델이 좋다는 것은 어떤 것인지를 정의해야 한다. 
  사실 머신러닝에서는 모델이 안좋다는 것이 어떤 의미인지를 주로 정의한다. 
2) 우리의 모델을 학습시킨다


 - 비용(cost), 손실(loss): 우리의 모델이 우리가 원하는 결과에서 얼마나 떨어져있는지를 보여주는 값
  우리는 그 격차를 줄이기 위해 노력하며, 그 격차가 적으면 적을수록 모델은 좋다

 - 크로스 엔트로피 : 모델의 손실을 정의하기 위해 자주 사용되는 함수 중 하나 
  \(y\) : 우리가 예측한 확률 분포
  \(y'\) : 실제 분포(우리가 입력하는 원-핫 벡터)
대략, 우리의 예측이 실제 값을 설명하기에 얼마나 비효율적인지를 측정하는 것

 - 올바른 답을 넣기 위한 새로운 placeholder를 추가하는 것
y_ = tf.placeholder(tf.float32, [None, 10])

 - 크로스 엔트로피 \(-\sum y'\log(y)\) 를 구현
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
 1) tf.log는 y의 각 원소의 로그 값을 계산한다.
 2) y_의 각 원소를 tf.log(y)의 해당하는 원소들과 곱한다.
 3)  tf.reduce_sum으로 y의 2번째 차원(reduction_indices=[1]이라는 파라미터가 주어졌으므로)의 원소들을 합한다.
 4) tf.reduce_mean으로 배치(batch)의 모든 예시에 대한 평균을 계산한다. 

 - 텐서플로우는 당신이 하고자 하는 연산의 전체 그래프를 알고 있으므로, 손실(당신이 최소화 하고 싶어하는 것)에 당신이 설정한 변수들이 어떻게 영향을 주는지를 역전파(backpropagation) 알고리즘을 자동으로 사용하여 매우 효율적으로 정의할 수 있다. 그리고나서 텐서플로우는 당신이 선택한 최적화 알고리즘을 적용하여 변수를 수정하고 손실을 줄일 수 있다.
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
 ->텐서플로우에게 학습 비율 0.5로 경사 하강법(gradient descent algorithm)을 적용하여 크로스 엔트로피를 최소화하도록 지시한다. 그러나 텐서플로우는 다른 여러 최적화 알고리즘을 제공한다. 그 중 하나를 적용하는 것은 코드 한 줄만 수정하면 될 정도로 간단하다.
 경사하강법: 텐서플로우가 각각의 변수를 비용을 줄이는 방향으로 조금씩 이동시키는 매우 단순한 방법



 - 텐서플로우가 실제로 뒤에서 하는 일: 역전파와 경사하강이라는 새로운 작업을 당신의 그래프에 추가하는 것이다. 텐서플로우가 실행되면 비용을 감소시키기 위해 변수들을 살짝 수정하는 경사 하강 학습 작업 한 번을 돌려줄 것이다.


 - 이제 모델은 학습할 준비가 되었다.  학습을 실행시키기 전, 변수들을 초기화 해야 한다.
init = tf.global_variables_initializer()


 - 이제 Session에서 모델을 실행시키고, 변수들을 초기화 하는 작업을 실행시킬 수 있다
sess = tf.Session()
sess.run(init)


 - 학습을 시킨다(1000번)
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

-> 배치(batch): 무작위로 선택된 100개의 데이터
1) 반복되는 루프의 각 단계마다, 학습 데이터셋에서 "배치"를 가져온다. 
2) placeholder의 자리에 데이터를 넣을 수 있도록 train_step을 실행하여 배치 데이터를 넘긴다.

 - 확률적 학습(stochastic training): 무작위 데이터의 작은 배치를 사용하는 방법 (여기서는 확률적 경사 하강법) 
 - 이상적으로는 학습의 매 단계마다 전체 데이터를 사용하고 싶지만(그렇게 하는게 우리가 지금 어떻게 하는게 좋을지에 대해 더 잘 알려줄 것이므로), 그렇게 하면 작업이 무거워진다. 
   따라서 그 대신에 매번 서로 다른 부분집합을 사용하는 것이다. 
   이렇게 하면 작업 내용은 가벼워지지만 전체 데이터를 쓸 때의 이점은 거의 다 얻을 수 있기 때문이다.






* 모델 평가하기
 - 우리가 작성한 모델의 성능 정도
1. 모델이 라벨을 올바르게 예측했는지 확인해본다. 
 - tf.argmax : 텐서 안에서 특정 축을 따라 가장 큰 값의 인덱스를 찾기에 매우 유용한 함수
 ex) tf.argmax(y,1): 우리의 모델이 생각하기에 각 데이터에 가장 적합하다고 판단한(가장 증거값이 큰) 라벨
      tf.argmax(y_,1): 실제 라벨

 - tf.equal : 예측이 맞았는지 확인할 수 있음
correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
-> 부울 값으로 이루어진 리스트를 얻게 된다.

2. 얼마나 많이 맞았는지 판단하려면, 이 값을 부동소수점 값으로 변환한 후 평균을 계산하면 된다.
 ex)  [True, False, True, True]는 [1,0,1,1]로 환산 가능, 이 값의 평균 = 0.75
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

3. 마지막으로, 우리의 테스트 데이터를 대상으로 정확도를 계산한다.
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
-> 결과는 약 92%
    우리가 매우 단순한 모델을 사용했기 때문에 매우 안좋은 결과이다. 
    약간만 바꾸면, 97%의 정확도를 얻을 수 있고, 가장 좋은 모델은 정확도가 99.7%도 넘을 수 있다.
